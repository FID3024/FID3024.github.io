---
layout: page
title: Schedule
permalink: /schedule/
---

# **Module 1: Fundamental of ML, e.g., scalability, batch size, etc.**
## **Lecture Session:** Sep. 28, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)] [[slides](/slides/1-1_ml_fundamental.pdf)] [[video](https://kth.box.com/s/orujm4n8iwhngv9k4ysou0qvwobkez6c)]
## **Discussion Session:** Oct. 5, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)] [[slides](/slides/1-2_ml_fundamental_discussion.pdf)] [[video](https://kth.box.com/s/8pvsw06z01h6ei6wmyq8twocvib6ln5n)]

#### **Required Reading**
* Measuring the Effects of Data Parallelism on Neural Network Training [[pdf](/papers/2019 - Measuring the Effects of Data Parallelism on Neural Network Training.pdf)]
* Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour [[pdf](/papers/2018 - Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour.pdf)]
* CROSSBOW: Scaling Deep Learning with Small Batch Sizes on Multi-GPU Servers [[pdf](/papers/2018 - CROSSBOW: Scaling Deep Learning with Small Batch Sizes on Multi-GPU Servers.pdf)]
* Don't Use Large Mini-Batches, Use Local SGD [[pdf](/papers/2020 - Don't Use Large Mini-Batches, Use Local SGD.pdf)]

#### **Optional Reading**
* Scaling SGD Batch Size to 32K for ImageNet Training [[pdf](/papers/2017 - Scaling SGD Batch Size to 32K for ImageNet Training.pdf)]

<br>
<hr>
<br>
# **Module 2: Distributed learning: data-parallelization**
## **Lecture Session:** Oct. 12, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)] [[slides](/slides/2-1_data_parallel.pdf)] [[video](https://kth.box.com/s/z31pw5giwiynpxlmw53gdujuhms59ngc)]
## **Discussion Session 2:** Oct. 19, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)]

#### **Required Reading**
* Communication-Efficient Distributed Deep Learning: A Comprehensive Survey [[pdf](/papers/2020 - Communication-Efficient Distributed Deep Learning: A Comprehensive Survey.pdf)]
* TicTac: Accelerating Distributed Deep Learning with Communication Scheduling [[pdf](/papers/2019 - TicTac: Accelerating Distributed Deep Learning with Communication Scheduling.pdf)]
* Caramel: Accelerating Decentralized Distributed Deep Learning with Computation Scheduling [[pdf](/papers/2020 - Caramel: Accelerating Decentralized Distributed Deep Learning with Computation Scheduling.pdf)]
* CodedReduce: A Fast and Robust Framework for Gradient Aggregation in Distributed Learning [[pdf](/papers/2019 - CodedReduce: A Fast and Robust Framework for Gradient Aggregation in Distributed Learning.pdf)]

#### **Optional Reading**
* More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server [[pdf](/papers/2013 - More Effective Distributed ML via a Stale Sychronous Parallel Parameter Server.pdf)]
* Scaling Distributed Machine Learning with the Parameter Server [[pdf](/papers/2014 - Scaling Distributed Machine Learning with the Parameter Server.pdf)]

<br>
<hr>
<br>
# **Module 3: Distributed learning: model-parallelization**
## **Lecture Session:** Oct. 26, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)]
## **Discussion Session:** Nov. 2, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)]

#### **Required Reading**
* The TensorFlow Partitioning and Scheduling Problem [[pdf](/papers/2017 - The TensorFlow Partitioning and Scheduling Problem.pdf)]
* Device Placement Optimization with Reinforcement Learning [[pdf](/papers/2017 - Device Placement Optimization with Reinforcement Learning.pdf)]
* A Hierarchical Model for Device Placement [[pdf](/papers/2018 - A Hierarchical Model for Device Placement.pdf)]
* Placeto: Learning Generalizable Device Placement Algorithms for Distributed Machine Learning [[pdf](/papers/2019 - Placeto: Learning Generalizable Device Placement Algorithms for Distributed Machine Learning.pdf)]
* GDP: Generalized Device Placement for Dataflow Graphs [[pdf](/papers/2019 - GDP: Generalized Device Placement for Dataflow Graphs.pdf)]
* Spotlight: Optimizing Device Placement for Training Deep Neural Networks [[pdf](/papers/2018 - Spotlight: Optimizing Device Placement for Training Deep Neural Networks.pdf)]

#### **Optional Reading**
* Graph Representation Matters in Device Placement [[pdf](/papers/2020 - Graph Representation Matters in Device Placement.pdf)]
* Inductive Representation Learning on Large Graphs [[pdf](/papers/2017 - Inductive Representation Learning on Large Graphs.pdf)]
* Position-aware Graph Neural Networks [[pdf](/papers/2019 - Position-aware Graph Neural Networks.pdf)]
* Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context [[pdf](/papers/2019 - Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context.pdf)]

<br>
<hr>
<br>
# **Module 4: Robust learning, e.g., byzantine-resilient learning**
## **Lecture Session:** Nov. 9, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)]
## **Discussion Session 4:** Nov. 16, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)]

#### **Required Reading**
* Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent [[pdf](/papers/2017 - Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent.pdf)]
* The Hidden Vulnerability of Distributed Learning in Byzantium [[pdf](/papers/2018 - The Hidden Vulnerability of Distributed Learning in Byzantium.pdf)]
* AGGREGATHOR: Byzantine Machine Learning via Robust Gradient Aggregation [[pdf](/papers/2019 - AGGREGATHOR: Byzantine Machine Learning via Robust Gradient Aggregation.pdf)]
* SGD: Decentralized Byzantine Resilience [[pdf](/papers/2019 - SGD: Decentralized Byzantine Resilience.pdf)]
* Fast Machine Learning with Byzantine Workers and Servers [[pdf](/papers/2019 - Fast Machine Learning with Byzantine Workers and Servers.pdf)]
* DRACO: Byzantine-resilient Distributed Training via Redundant Gradients [[pdf](/papers/2018 - DRACO: Byzantine-resilient Distributed Training via Redundant Gradients.pdf)] 

#### **Optional Reading**
* Generalized Byzantine-tolerant SGD [[pdf](/papers/2018 - Generalized Byzantine-tolerant SGD.pdf)]

<br>
<hr>
<br>
# **Module 5: AutoML, e.g., hyperparameter optimization, meta learning, and neural architecture search**
## **Lecture Session:** Nov. 23, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)]
## **Discussion Session:** Nov. 30, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)]

#### **Required Reading**
* TBA

#### **Optional Reading**
* TBA

<br>
<hr>
<br>
# **Module 6: ML platforms, e.g., TensorFlow, Ray, Mllib**
## **Lecture Session:** Dec. 7, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)]
## **Discussion Session:** Dec. 14, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)]

#### **Required Reading**
* TBA

#### **Optional Reading**
* TBA
