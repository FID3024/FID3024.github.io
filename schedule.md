---
layout: page
title: Schedule
permalink: /schedule/
---

# **Module 1: Fundamental of ML, e.g., scalability, batch size, etc.**
## **Lecture Session:** Sep. 28, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)] [[slides](/slides/1-1_ml_fundamental.pdf)] [[video](https://kth.box.com/s/orujm4n8iwhngv9k4ysou0qvwobkez6c)]
## **Discussion Session:** Oct. 5, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)] [[slides](/slides/1-2_ml_fundamental_discussion.pdf)] [[video](https://kth.box.com/s/8pvsw06z01h6ei6wmyq8twocvib6ln5n)]

#### **Required Reading**
* Measuring the Effects of Data Parallelism on Neural Network Training [[pdf](/papers/2019 - Measuring the Effects of Data Parallelism on Neural Network Training.pdf)]
* Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour [[pdf](/papers/2018 - Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour.pdf)]
* CROSSBOW: Scaling Deep Learning with Small Batch Sizes on Multi-GPU Servers [[pdf](/papers/2018 - CROSSBOW: Scaling Deep Learning with Small Batch Sizes on Multi-GPU Servers.pdf)]
* Don't Use Large Mini-Batches, Use Local SGD [[pdf](/papers/2020 - Don't Use Large Mini-Batches, Use Local SGD.pdf)]

#### **Optional Reading**
* Scaling SGD Batch Size to 32K for ImageNet Training [[pdf](/papers/2017 - Scaling SGD Batch Size to 32K for ImageNet Training.pdf)]

<br>
<hr>
<br>
# **Module 2: Distributed learning: data-parallelization**
## **Lecture Session:** Oct. 12, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)] [[slides](/slides/2-1_data_parallel.pdf)] [[video](https://kth.box.com/s/z31pw5giwiynpxlmw53gdujuhms59ngc)]
## **Discussion Session 2:** Oct. 19, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)] [[slides](/slides/2-2_data_parallel.pdf)] [[video](https://kth.box.com/s/jiyj2pvr7zhy1n2yni7ol6kjcs1r8o0k)]

#### **Required Reading**
* Communication-Efficient Distributed Deep Learning: A Comprehensive Survey [[pdf](/papers/2020 - Communication-Efficient Distributed Deep Learning: A Comprehensive Survey.pdf)]
* TicTac: Accelerating Distributed Deep Learning with Communication Scheduling [[pdf](/papers/2019 - TicTac: Accelerating Distributed Deep Learning with Communication Scheduling.pdf)]
* Caramel: Accelerating Decentralized Distributed Deep Learning with Computation Scheduling [[pdf](/papers/2020 - Caramel: Accelerating Decentralized Distributed Deep Learning with Computation Scheduling.pdf)]
* CodedReduce: A Fast and Robust Framework for Gradient Aggregation in Distributed Learning [[pdf](/papers/2019 - CodedReduce: A Fast and Robust Framework for Gradient Aggregation in Distributed Learning.pdf)]

#### **Optional Reading**
* More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server [[pdf](/papers/2013 - More Effective Distributed ML via a Stale Sychronous Parallel Parameter Server.pdf)]
* Scaling Distributed Machine Learning with the Parameter Server [[pdf](/papers/2014 - Scaling Distributed Machine Learning with the Parameter Server.pdf)]
* MG-WFBP: Efficient Data Communication for Distributed Synchronous SGD Algorithms [[pdf](/papers/2018 - MG-WFBP: Efficient Data Communication for Distributed Synchronous SGD Algorithms.pdf)]
* Gradient Coding: Avoiding Stragglers in Distributed Learning [[pdf](/papers/2017 - Gradient Coding: Avoiding Stragglers in Distributed Learning.pdf)]
* Asynchronous Decentralized Parallel Stochastic Gradient Descent [[pdf](/papers/2018 - Asynchronous Decentralized Parallel Stochastic Gradient Descent.pdf)]
* GossipGraD: Scalable Deep Learning using Gossip Communication based Asychronous Gradient Descent [[pdf](/papers/2018 - GossipGraD: Scalable Deep Learning using Gossip Communication based Asychronous Gradient Descent.pdf)]

<br>
<hr>
<br>
# **Module 3: Distributed learning: model-parallelization**
## **Lecture Session:** Oct. 26, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)] [[slides](/slides/3-1_model_parallel.pdf)] [[video](https://kth.box.com/s/mt9y1a227gnkjfd1ml3sf9hwn3d0595r)]
## **Discussion Session:** Nov. 2, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)] [[slides](/slides/3-2_model_parallel.pdf)]

#### **Required Reading**
* The TensorFlow Partitioning and Scheduling Problem [[pdf](/papers/2017 - The TensorFlow Partitioning and Scheduling Problem.pdf)]
* Device Placement Optimization with Reinforcement Learning [[pdf](/papers/2017 - Device Placement Optimization with Reinforcement Learning.pdf)]
* A Hierarchical Model for Device Placement [[pdf](/papers/2018 - A Hierarchical Model for Device Placement.pdf)]
* Placeto: Learning Generalizable Device Placement Algorithms for Distributed Machine Learning [[pdf](/papers/2019 - Placeto: Learning Generalizable Device Placement Algorithms for Distributed Machine Learning.pdf)]
* A Single-Shot Generalized Device Placement for Large Dataflow Graphs [[pdf](/papers/2020 - A Single-Shot Generalized Device Placement for Large Dataflow Graphs.pdf)]
* Spotlight: Optimizing Device Placement for Training Deep Neural Networks [[pdf](/papers/2018 - Spotlight: Optimizing Device Placement for Training Deep Neural Networks.pdf)]

#### **Optional Reading**
* GDP: Generalized Device Placement for Dataflow Graphs [[pdf](/papers/2019 - GDP: Generalized Device Placement for Dataflow Graphs.pdf)]
* Post: Device Placement with Cross-Entropy Minimization and Proximal Policy Optimization [[pdf](/papers/2018 - Post: Device Placement with Cross-Entropy Minimization and Proximal Policy Optimization.pdf)]
* Graph Representation Matters in Device Placement [[pdf](/papers/2020 - Graph Representation Matters in Device Placement.pdf)]
* Inductive Representation Learning on Large Graphs [[pdf](/papers/2017 - Inductive Representation Learning on Large Graphs.pdf)]
* Position-aware Graph Neural Networks [[pdf](/papers/2019 - Position-aware Graph Neural Networks.pdf)]
* Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context [[pdf](/papers/2019 - Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context.pdf)]

<br>
<hr>
<br>
# **Module 4: Robust learning, e.g., byzantine-resilient learning**
## **Lecture Session:** Nov. 9, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)] [[slides](/slides/4-1_robust_learning.pdf)] [[video](https://kth.box.com/s/k9zk9zg058ddn92vohkvn0ysuhlqbgtz)]
## **Discussion Session 4:** Nov. 16, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)] [[slides](/slides/4-2_robust_learning.pdf)]

#### **Required Reading**
* Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent [[pdf](/papers/2017 - Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent.pdf)]
* The Hidden Vulnerability of Distributed Learning in Byzantium [[pdf](/papers/2018 - The Hidden Vulnerability of Distributed Learning in Byzantium.pdf)]
* AGGREGATHOR: Byzantine Machine Learning via Robust Gradient Aggregation [[pdf](/papers/2019 - AGGREGATHOR: Byzantine Machine Learning via Robust Gradient Aggregation.pdf)]
* SGD: Decentralized Byzantine Resilience [[pdf](/papers/2019 - SGD: Decentralized Byzantine Resilience.pdf)]
* Fast Machine Learning with Byzantine Workers and Servers [[pdf](/papers/2019 - Fast Machine Learning with Byzantine Workers and Servers.pdf)]
* DRACO: Byzantine-resilient Distributed Training via Redundant Gradients [[pdf](/papers/2018 - DRACO: Byzantine-resilient Distributed Training via Redundant Gradients.pdf)] 

#### **Optional Reading**
* Generalized Byzantine-tolerant SGD [[pdf](/papers/2018 - Generalized Byzantine-tolerant SGD.pdf)]
* SoK: Security and Privacy in Machine Learning [[pdf](/papers/2018 - SoK: Security and Privacy in Machine Learning.pdf)]

<br>
<hr>
<br>
# **Module 5: AutoML, e.g., hyperparameter optimization, meta learning, and neural architecture search**
## **Lecture Session:** Nov. 23, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)]
## **Discussion Session:** Nov. 30, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)]

#### **Required Reading**
* Automated Machine Learning: State-of-The-Art and Open Challenges [[pdf](/papers/2019 - Automated Machine Learning: State-of-The-Art and Open Challenges.pdf)]
* BOHB: Robust and Efficient Hyperparameter Optimization at Scale [[pdf](/papers/2018 - BOHB: Robust and Efficient Hyperparameter Optimization at Scale.pdf)]
* A System for Massively Parallel Hyperparameter Tuning [[pdf](/papers/2020 - A System for Massively Parallel Hyperparameter Tuning.pdf)]
* DARTS: Differentable Architecture Search [[pdf](/papers/2019 - DARTS: Differentable Architecture Search.pdf)]
* ASAP: Architecture Search, Anneal and Prune [[pdf](/papers/2020 - ASAP: Architecture Search, Anneal and Prune.pdf)]

#### **Optional Reading**
* AutoML Book - Hyperparameter Optimization [[pdf](/papers/2018 - AutoML Book - Hyperparameter Optimization.pdf)]
* AutoML Book - Meta-Learning [[pdf](/papers/2018 - AutoML Book - Meta-Learning.pdf)]
* AutoML Book -  Neural Architecutre Search [[pdf](/papers/2018 - AutoML Book -  Neural Architecutre Search.pdf)]
* Non-stochastic Best Arm Identification and Hyperparameter Optimization [[pdf](/papers/2015 - Non-stochastic Best Arm Identification and Hyperparameter Optimization.pdf)]
* Hyperband: A Novel Bandit-based Approach to Hyperparameter Optimization [[pdf](/papers/2018 - Hyperband: A novel bandit-based approach to hyperparameter optimization.pdf)]
* Random Search and Reproducibility for Neural Architecture Search [[pdf](/papers/2020 - Random Search and Reproducibility for Neural Architecture Search.pdf)]
* Maggy: Scalable Asynchronous Parallel Hyperparameter Search [[pdf](/papers/2020 - Maggy: Scalable Asynchronous Parallel Hyperparameter Search.pdf)]

<br>
<hr>
<br>
# **Module 6: ML platforms, e.g., BigDL, PyTorch Distributed, ZeRO**
## **Lecture Session:** Dec. 7, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)]
## **Discussion Session:** Dec. 14, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)]

#### **Required Reading**
* BigDL: A Distributed Deep Learning Framework for Big Data [[pdf](/papers/2019 - BigDL: A Distributed Deep Learning Framework for Big Data.pdf)]
* PyTorch Distributed: Experiences on Accelerating Data Parallel Training.pdf [[pdf](/papers/2020 - PyTorch Distributed: Experiences on Accelerating Data Parallel Training.pdf)]
* ZeRO: Memory Optimizations Toward Training Trillion Parameter Models.pdf [[pdf](/papers/2020 - ZeRO: Memory Optimizations Toward Training Trillion Parameter Models.pdf)]
* Beyonf Data and Model Parallelism for Deep Neural Networks [[pdf](/papers/2019 - Beyonf Data and Model Parallelism for Deep Neural Networks.pdf)]

#### **Optional Reading**
* Caffe: Convolutional Architecture for Fast Feature Embedding [[pdf](/papers/2014 - Caffe: Convolutional Architecture for Fast Feature Embedding.pdf)]
* MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems [[pdf](/papers/2015 - MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems.pdf)]
* TensorFlow: A system for large-scale machine learning [[pdf](/papers/2016 - TensorFlow: A system for large-scale machine learning.pdf)]
* Horovod: fast and easy distributed deep learning in TensorFlow [[pdf](/papers/2018 - Horovod: fast and easy distributed deep learning in TensorFlow.pdf)]
* Mesh-TensorFlow: Deep Learning for Supercomputers [[pdf](/papers/2018 - Mesh-TensorFlow: Deep Learning for Supercomputers.pdf)]
* MXNET-MPI: Embedding MPI parallelism in Parameter Server Task Model for Scaling Deep Learning [[pdf](/papers/2018 - MXNET-MPI: Embedding MPI parallelism in Parameter Server Task Model for Scaling Deep Learning.pdf)]
* PyTorch: An Imperative Style, High-Performance [[pdf](/papers/2019 - PyTorch: An Imperative Style, High-Performance.pdf)]
* HyPar-Flow: Exploiting MPI and Keras for Scalable Hybrid-Parallel DNN Training using TensorFlow [[pdf](/papers/2020 - HyPar-Flow: Exploiting MPI and Keras for Scalable Hybrid-Parallel DNN Training using TensorFlow.pdf)]
* Towards a Scalable and Distributed Infrastructure for Deep Learning Applications [[pdf](/papers/2020 - Towards a Scalable and Distributed Infrastructure for Deep Learning Applications.pdf)]
