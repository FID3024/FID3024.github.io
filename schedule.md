---
layout: page
title: Schedule
permalink: /schedule/
---

# Lecture Session 1: Sep. 28, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)]
#### **Topic: Fundamental of ML, e.g., scalability, batch size, etc..**
#### **Reading material**
* Measuring the Effects of Data Parallelism on Neural Network Training [[pdf](/papers/2019 - Measuring the Effects of Data Parallelism on Neural Network Training.pdf)]
* Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour [[pdf](/papers/2018 - Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour.pdf)]
* CROSSBOW: Scaling Deep Learning with Small Batch Sizes on Multi-GPU Servers [[pdf](/papers/2018 - CROSSBOW: Scaling Deep Learning with Small Batch Sizes on Multi-GPU Servers.pdf)]
* Don't Use Large Mini-Batches, Use Local SGD [[pdf](/papers/2020 - Don't Use Large Mini-Batches, Use Local SGD.pdf)]

<br>
<hr>
<br>
# Discussion Session 1: Oct. 5, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)]
#### **Topic: Fundamental of ML, e.g., scalability, batch size, etc.**
#### **Reading material**
* Communication-Efficient Distributed Deep Learning: A Comprehensive Survey [[pdf](/papers/2020 - Communication-Efficient Distributed Deep Learning: A Comprehensive Survey.pdf)]
* TicTac: Accelerating Distributed Deep Learning with Communication Scheduling [[pdf](/papers/2019 - TicTac: Accelerating Distributed Deep Learning with Communication Scheduling.pdf)]
* Caramel: Accelerating Decentralized Distributed Deep Learning with Computation Scheduling [[pdf](/papers/2020 - Caramel: Accelerating Decentralized Distributed Deep Learning with Computation Scheduling.pdf)]
* CodedReduce: A Fast and Robust Framework for Gradient Aggregation in Distributed Learning [[pdf](/papers/2019 - CodedReduce: A Fast and Robust Framework for Gradient Aggregation in Distributed Learning.pdf)]

<br>
<hr>
<br>
# Lecture Session 2: Oct. 12, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)]
#### **Topic: Distributed learning: data-parallelization**
#### **Reading material**
* TBA

<br>
<hr>
<br>
# Discussion Session 2: Oct. 19, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)]
#### **Topic: Distributed learning: data-parallelization**
#### **Reading material**
* TBA

<br>
<hr>
<br>
# Lecture Session 3: Oct. 26, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)]
#### **Topic: Distributed learning: model-parallelization**
#### **Reading material**
* TBA

<br>
<hr>
<br>
# Discussion Session 3: Nov. 2, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)]
#### **Topic: Distributed learning: model-parallelization**
#### **Reading material**
* TBA

<br>
<hr>
<br>
# Lecture Session 4: Nov. 9, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)]
#### **Topic: Robust learning, e.g., byzantine-resilient learning**
#### **Reading material**
* TBA

<br>
<hr>
<br>
# Discussion Session 4: Nov. 16, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)]
#### **Topic: Robust learning, e.g., byzantine-resilient learning**
#### **Reading material**
* TBA

<br>
<hr>
<br>
# Lecture Session 5: Nov. 23, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)]
#### **Topic: AutoML, e.g., hyperparameter optimization, meta learning, and neural architecture search**
#### **Reading material**
* TBA

<br>
<hr>
<br>
# Discussion Session 5: Nov. 30, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)]
#### **Topic: AutoML, e.g., hyperparameter optimization, meta learning, and neural architecture search**
#### **Reading material**
* TBA

<br>
<hr>
<br>
# Lecture Session 6: Dec. 7, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)]
#### **Topic: ML platforms, e.g., TensorFlow, Ray, Mllib**
#### **Reading material**
* TBA

<br>
<hr>
<br>
# Discussion Session 6: Dec. 14, 15:00-17:00 [[zoom](https://kth-se.zoom.us/j/2884945301)]
#### **Topic: ML platforms, e.g., TensorFlow, Ray, Mllib**
#### **Reading material**
* TBA
